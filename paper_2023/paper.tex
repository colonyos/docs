\documentclass{article}

\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}

\title{Colonies - Compute Continuums across Platforms}

\author{{\hspace{1mm}Johan Kristiansson} \\
	Department of Computer Science \\
	RISE Research Institutes of Sweden \\
	Luleå, Sweden \\
	\texttt{johan.kristiansson@ri.se} \\
	\And
	{\hspace{1mm}Thomas Ohlson Timoudas} \\
	Department of Computer Science \\
	RISE Research Institutes of Sweden \\
	Luleå, Sweden \\
	\texttt{thomas.ohlson.timoudas@ri.se} \\
	\And
	{\hspace{1mm}Henrik Forsgren} \\
	Department of Computer Science \\
	RISE Research Institutes of Sweden \\
	Luleå, Sweden \\
	\texttt{thomas.ohlson.timoudas@ri.se} \\
	\And
	{\hspace{1mm}Erik Källman} \\
	Department of Computer Science \\
	RISE Research Institutes of Sweden \\
	Luleå, Sweden \\
	\texttt{erik.kallman@ri.se} \\
}

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{Technical Report}
\renewcommand{\undertitle}{Technical Report}
%\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
Artificial intelligence and machine learning has gained significant traction in recent years. At the same time, development and operation of AI workloads has become increasingly challenging. One difficulty is the lack of portability, making it cumbersome to move from one platform to another. Creating and operating fully automated end-to-end workflows across devices, edge, and cloud platforms is even more challenging.

To address the aforementioned challenges, the paper introduces an open-source framework called Colonies\footnote{https://github.com/colonyos/colonies}, which is designed to facilitate execution of computational workloads across a diverse range of platforms. Colonies enables development of a \emph{grid-like distributed virtual computer}, called a \emph{Colony}, which can effortlessly integrate with any third-party application or other workflow management systems.

Built upon a stateless microservice architecture, Colonies enables complex workflows to be broken down into composable functions. With the use of an HTTP protocol, these composable functions can be implemented in any programming language and be executed by independent executors deployed across platforms, anywhere on the Internet, including cloud, edge, high-performance computing (HPC) environments, devices, or even web browsers. A zero-trust security protocol enables a collection of distributed executors to operate as a unified entity, thus establishing seamless computational continuums across multiple platforms.

In addition to a technical description of the Colonies framework, the paper also describes some potential use cases. The paper describe how Colonies can be leveraged to build a remote sensing platform on Kubernetes, serve as a building block for edge computing, implement a serverless FaaS (Function-as-a-Service), and how it can be integrated with the Slurm workload manager. Finally, the paper presents a performance investigation, as well as scalability and robustness evaluation. 

In summary, Colonies is a highly versatile and scalable framework that can streamline development and deployment of computational workloads across heterogeneous platforms while ensuring full traceability and zero-trust security.
\end{abstract}

% keywords can be removed
\keywords{Distributed computing, serverless computing \and parallel computing \and Workflow orchestration}

\section{Introduction}
Developing robust and scalable AI systems is a challenging task that requires deep understanding in several fields. To begin with, an AI model must be trained which requires knowledge in advanced statistics or machine learning. Typically, training and validation data must be pre-processed through various stages before it can be utilized. Although it may be practical for small-scale projects to run the entire training processes on local development computers, larger AI models typically require access to powerful compute clusters or even high-performance computing (HPC) systems. Manual use of such infrastructure can be laborious and time-consuming. Automating the training process enables faster iterations and quicker discovery of useful models.

Taking an AI model into production requires substantial software engineering expertise and collaboration across teams. In contrast to traditional IT workloads, both the data and the model must be managed in addition to the software itself. As most models require regular re-training or re-calibration, it must be possible to update deployed models and software seamlessly without losing information or breaking the system. In many cases, there is a constant flow of data ingested into the system which must be managed even in case of failures. This becomes even more challenging when nodes or parts of the underlying infrastructure become unavailable due to maintenance such as software updates, hardware replacements and sometimes missconfiguration problems.

In some cases, it may be necessary to scale the system to increase or reduce the capacity. This is especially critical when using expensive cloud resources. Scaling the system means that the underlying infrastructure may change at any time, causing instability issues for running services or workflows. Therefore, it must be possible to detect failed computations and reprocess failed tasks part of a larger workflow. Workflows must hence be designed to handle an ever-changing infrastructure, and if a failed computation cannot be restored gracefully, engineers must be able to quickly perform root cause analysis to manually recover the system.

In reality, AI system requires integration of multiple systems. For instance, data need to be captured from an IoT system or pulled from third-party database running on different domains than the compute cluster itself. With the emergence of edge computing, parts of a data pipeline may also run on edge servers to bring computations closer to data sources. Configuring and setting up such pipelines add even more complexity. 

Additionally, many compute clusters operate on-premises installations. Sometimes it is necessary to temporarily increase the capacity of on-prem clusters by combining resources from multiple providers, for example, adding cloud compute resources to handle peak loads or utilize HPC resources to quickly reprocess historical data. Developing hybrid workflows where some jobs run in the cloud and others run on HPC systems requires even more software development efforts \cite{wf_challenges} and is beyond the scope of many users, preventing them from utilizing powerful hardware. Clearly, there is a need for a framework that can consolidate various workflow management platforms to simplify development and enable seamless execution across platforms.

This paper presents a framework called Colonies, specifically designed to implement a \emph{distributed virtual computer system} that can easily integrate with any third-party system. Colonies is built around a loosely-coupled microservices architecture that separates workflow definitions from implementation and deployment. The primary objective is to establish a platform where monolithic workflows can be decomposed into independent microservices that can easily be integrated with other systems or applications running anywhere on the Internet. The remainder of the paper describes the Colonies framework and how it can be used to create robust and scalable AI systems across plattforms. 

\section{Related work}
Workflow management has been extensively studied in both academic and industrial settings with numerous approaches \cite{service_wfs, schmitt2022workflow, GarciaRepresa1740746, Ouyang2010, NIKOLOV2021100440, workflow_in_bigdata} proposed to address the challenges in this field. Recently, Apache Airflow \cite{apache_airflow} has become a popular open-source workflow management system for handling data engineering pipelines. Like Colonies, Apache Airflow enables developers to create custom operators and executors that can be integrated with various systems. Additionally, Apache Airflow offers an HTTP API that makes it possible to develop software development kits (SDKs) in various programming languages. However, Apache Airflow does not rely on a queuing system. Instead, it must be integrated with a message broker, such as RabbitMQ \cite{rabbitmq} or Kafka \cite{apache_kafka}, to implement task queues, resulting in a more complex architecture than Colonies. Furthermore, Colonies is based on a distributed microservice architecture that makes it more suitable for DevOps software development. As Colonies is loosely coupled, executors can be implemented and dynamically deployed on the Internet without reconfiguring the workflow engine.

Argo \cite{argowf} is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. It is can be used for running CI/CD pipelines or compute intensive machine learning or data processing tasks where each job runs as a container. In contrast, Colonies offers a more versatile approach, allowing jobs to be launched within an already started container. As launching new containers on Kubernetes can occasionally be time-consuming, Colonies can delivers higher throughput as the costs of starting new jobs are minimal. This is particular useful when launching large container images, or workloads (e.g. Julia scripts) that can take long time to start. 

Today, utilization of serverless computing is experiencing a significant growth \cite{cognit}, primarily due to its potential to liberate developers from the burden of managing underlying cloud infrastructures. Attempt are currently being made to implement serverless workflow management systems. For example, OpenWolf \cite{openwolf} is a serverless workflow engine designed to utilize the Function-as-a-Service (FaaS) paradigm for composing complex scientific workflows. It is based on OpenFaaS \cite{openfaas}, which allows functions to run on Kubernetes clusters. 
The serverless workflow project \cite{serverlessworkflows} aims at providing a vendor-neutral workflow DSL. Synapse \cite{synapse} is a workflow management system similar to Argo, but that implements the serverless workflow specification. A similar engine is proposed in \cite{scafe}. 
Colonies can also be used to implement serverless workflow management systems. This will be further discussed in Section \ref{faas}. However, in contrast to previous work, Colonies is plattform independent and does not require Kubernetes. By using a zero-trust security protocol, functions can be securely executed by distributed executor deployed anywhere on the Internet. It is important to point out that Colonies does not provide an infrastructure for function execution. Instead, Colonies primary role is to serve as a platform for coordinating function executions which are carried out by distributed executor.

Currently, microservices are primarily used to implement large-scale web applications or Internet applications requiring high-availability. It has not yet become a prevalent design principle for workload management or implementation of HPC applications. Instead, simple job scripts are commonly used. J. Represa et al. \cite{GarciaRepresa1740746, GarciaRepresa1640771} explore various challenges associated with developing microservice-based workflow management for industrial automation within the context of the Arrowhead project \cite{delsing2017iot}. The authors conclude that microservice-based workflow technologies is viable for industrial applications, particularly due to their inherent flexibility. The primary contribution of this paper is a comprehensive technical description of how to implement a distributed workflow engine based on microservice principles, extending beyond orchestrating microservices to execute automation tasks within one platform. The vision is to create a distributed architecuture where microservices can reside anywhere on the Internet and still function as a cohesive unit to execute cross-platform workflows.  

Grid computing \cite{grid_computing} is a distributed computing model that allows multiple computers, which may be geographically dispersed, to collaborate in addressing large-scale computational challenges. The Colonies framework is founded upon a grid computing model with the primary purpose of assigning tasks to distributed executors. To the best of the authors' knowledge, no previous work has integrated a microservice-oriented architecture with a grid computing model to serve as an integration point for coordinating artificial intelligence workloads across a diverse range of platforms. Additionally, Colonies is designed to function as a ledger, offering complete transparency and execution history, which is essential for implementing zero-trust security.

\begin{figure}[t]
	\centering
    \includegraphics[scale=0.45]{overview.png}
	\caption{Overview of the Colonies framework. Executors may be deployed anywhere on the Internet.}
	\label{fig:overview}
\end{figure}

\section{The Colonies framework}
\label{sec:headings}
A core concept of the Colonies framework is the notion of \emph{processes}. A process contains meta-information about computations executed by a remote computer programs, which are referred to as \emph{Distributed executors}. Specifically, a process contains a definition of a function to be executed, as well as contextual information such as execution status, including the result of the computation. One can think of a process as a digital twin of a real computing operation process. It is essential to note that a process does not necessarily have to be an operating system process; it can represent any type of computation, e.g. a remote procedure call executed by any kind of software. 

Figure 1 depicts an overview of the components part of the Colonies framework. The \emph{Colonies servers} form the backbone of the framework, functioning as a centralized control system for task submission and assignment. The Colonies server acts as a job broker for executors, almost like an employment agency for people. All execution information and history are stored in a database maintained by the Colonies servers. Upon submission, a process is stored in the database, serving as a queueing system. When a process is assigned to an executor its execution status is changed from \emph{waiting} to \emph{running}. Consequently, it is possible to submit a process that should run in the future even if no executors can run the process at the moment. In this way, Colonies supports both batch and real-time processing. 

\emph{Distributed executors} are responsible for executing processes assigned by the Colonies Servers. Several distributed executors form a \emph{Colony}, which is a collection of executors operating as a cohesive unit where each executor is responsible for executing specific type of processes. One could view a Colony as an organization or a society of distributed computer softwares acting a single virtual computer system. To interact with other executors, executors must prove their Colony membership using a cryptographic protocol that follows the zero-trust security principle \emph{never trust, always verify}. This security protocol ensures that users can keep control even when executors are distributed across plattforms. Zero-trust security is fundamental to the Colonies framework and will be further discussed in Section \ref{zerotrustsecurity}. The following sections outlines the underlying design principles and describes the Colonies framework in more details. 

\subsection{Microservices}
Microservices is an architectural design pattern in which an application is structured as a collection of small, independently deployable, and loosely coupled services that communicate with other microservices through a well-defined API. By dividing the application into smaller, focused microservices, applications become easier to understand, maintain, and develop. In the Colonies framework, executors are microservices having the following characteristics:

\begin{itemize}
\item \textbf{Single responsibility:} Each executor is only accountable for executing specific functions. This makes the system easier to understand, develop, test and maintain.  
\item \textbf{Loosely coupled:} Executors are designed with minimal dependencies on other executors, enabling various software development teams to work independently. For instance, a data engineering team may handle the implementation of pre-processing functions, while a data science team oversees machine learning functions, and another team manages visualization or customer integration etc.
\item \textbf{Scalability:} Executors can be deployed independently, which enables horizontal scaling. This allows for better resource utilization, parallelism, and improved performance. 
\item \textbf{Resilience:} The failure of a single executor does not compromise the entire application or workflow. Executors' isolation from one another contributes to a more resilient and fault-tolerant system. If an executor crashes during execution, the process is automatically reassigned to another executor.
\item \textbf{DevOps and Continuous Integration:} Executors' inherent fault-tolerant design permits changes to individual executors without impacting the entire system. This makes it possible to seamlessly update the system. 
\item \textbf{Technology agnostic:} Executors can be implemented in any programming language, facilitating seamless integration with other applications and systems.
\item \textbf{Decentralized governance:} As the Colonies framework is technology-agnostic, different software development teams can make independent technology and design decisions when developing executors, promoting greater flexibility and adaptability. For example, some executors may be implemented in Rust, while others may use Python to leverage state-of-the-art machine learning frameworks.
\end{itemize}

Although microservices has attractive properties and simplifes both development and deployment of executors, they also introduce complexity to the Colonies servers. To implement a workflow management framework supporting distributed microservices, the following challenges must be addressed:

\begin{itemize}
\item \textbf{Process management:} Colonies must be able to distribute processes among available executors based on their capabilities and current workload. This involves assigning processes to the most appropriate executor and then monitor process progress.

\item \textbf{Fault tolerance and recovery:} In the event of executor failures, the Colonies framework must be able to re-assign processes to other executors, manage executor restarts, or trigger recovery mechanisms to maintain system reliability and resilience. In addition, the Colonies framework must be able to handle restarts or crashes of Colonies servers, thereby preventing any internal states in Colonies from becoming corrupted. 

\item \textbf{Load balancing:} The Colonies framework must manage load balancing among executors to optimize performance and avoid overloading any single executor.

\item \textbf{Service discovery:} The Colonies framework must enable executors to dynamically register and deregister to Colonies servers. It must be possible to deploy executors anywhere on the Internet, even behind firewalls.

\item \textbf{Workflow orchestration:} The Colonies framework must coordinate and orchestrate complex multi-step workflows executed by several executors, sometimes in parallel where executors run on different plattforms. Colonies must define the sequence in which processes should be executed, manages dependencies among processes, and ensure that workflows execute successfully to completion.

\item \textbf{Monitoring and debugging:} The Colonies framework must monitor the overall system, allowing administrators to track the health, performance, and resource utilization of the executors and the system as a whole.
\end{itemize}

The Colonies framework uses a combination of technologies to address the aforementioned challenges. A fundamental design principle is stateless web serices, which makes Colonies simpler, more reliable and scalable, and easier to implement. To ensure reliability and data consistency, distributed consensus algorithms are needed to offer high-availability and handle server crashes. The remainder of this section describes how the Colonies framework is implemented. The next discusses the role of queues to support batch jobs and realtime processing, but also how queues can be used to enable loose coupling of system components.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[scale=0.37]{arch.png}
         \caption{Process assignment steps.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[scale=0.37]{process.png}
         \caption{Manipulation of a process.}
     \end{subfigure}
     \caption{Process management via Colonies HTTP API. Note that only the assigned executor has write access to the process database entry.}
     \label{fig:process_assignment}
\end{figure}

\subsection{The role of queues as seperation of concerns}
Separation of concerns (SoC) is a design principle to break down a complex software system into smaller, more manageable parts. For example, HTTP APIs can be used to abstract away implementation detail and provide a clear and simple interface for interacting with a particular service. However, HTTP protocols alone are insufficient for handling dynamic environments where components frequently fails or the underlying infrastructures is constantly changing. To address such environments, an alternative mechanism is necessary. 

Queues enable software services to communicate indirectly by acting as a buffer between them. Queues makes it possible to decouple each executor and make them operate independently, e.g. an executor can be updated or replaced without affecting other executors. Queues also allow for asynchronous communication between executors, enabling them to process tasks at their own pace. This ensures that slower executors do not bottleneck faster ones, leading to a more efficient and scalable system. Most importantely, queues enable load balancing by distributing tasks among multiple executors, thus making it possible to parallelize workflow execution. 

Queues can be implemented in different ways, and while message brokers are a common solution, Colonies adopts an alternative strategy and leverage a standard database and querying it for tasks to assign to different executors. One key advantage of this approach is that it enables fine-grained process assignments, making it possible to assign specific processes to particular executors. For instance, an executor of the browser type can be limited to only executing processes in web applications. This level of granularity cannot easily be implemented using message brokers which generally do not offer introspection of queue, or provide the ability to pull specific messages out of the queue. Generarally, the only way to retrieve a specific message is to pull all messages from the queue, obtain the message, and then place all remaining messages back into the queue in the same order. In contrast, a database can function as a queue and a query can match any columns thus making it possible to assign specific executors to specific processes.

\subsection{Process tables}
Colonies enables executors to interact with each other by submitting function specifications to the Colonies servers. Once submitted, other executors can connect to the servers to receive process execution assignments. A new process entry is added to the process table database when a function specification is submitted to the Colonies server.

\begin{table}[t]
	\caption{Process Table}
	\centering
	\begin{tabular}{llllll}
		\toprule
		\cmidrule(r){1-2}
        Process Id & Function Spec & Wait for Parents & Assigned Executor Id & State      & Priority Time \\
		\midrule
        $P_{1}$    & $F_{1}$       & $False$          & $E_{1}$              & Successful & 1679906715352024000 \\
        $P_{2}$    & $F_{2}$       & $False$          & $E_{1}$              & Running    & 1679906715353453000 \\
        $P_{3}$    & $F_{3}$       & $False$          & $E_{2}$              & Running    & 1679906715354286000 \\
        $P_{4}$    & $F_{4}$       & $True$           & -                    & Waiting    & 1679906715355188000 \\
		\bottomrule
	\end{tabular}
	\label{proctable}
\end{table}

\begin{table}[t]
	\caption{Function Specifications}
	\centering
	\begin{tabular}{llllll}
		\toprule
		\cmidrule(r){1-2}
        Function Spec & Function        & Executor Type & Priority & Max Exec Time & Max Retries \\
		\midrule
        $F_{1}$       & gen\_nums()     & Edge          & 1        & 200 s         & 5 \\
        $F_{2}$       & square()        & Cloud         & 1        & 200 s         & 5 \\
        $F_{3}$       & square()        & Cloud         & 1        & 200 s         & 5 \\
        $F_{4}$       & sum()           & Browser       & 1        & 200 s         & 5 \\
		\bottomrule
	\end{tabular}
	\label{functable}
\end{table}

When an executor connects to the Colonies server, the server hang the incomming HTTP connection\footnote{An alternative protocol is to use WebSockets or gPRC to communicate with the Colonies server.} until the executor is assigned a process, or until a connection timer expires. Note that the Colonies server does not connect to the executors. Rather, it is the responsibility of the executors to connect to the Colonies server. This enables executors to be deployed anywhere on the Internet, behind firewalls, commercial telco networks, or even in web browser-based applications.

\begin{figure}[h]
	\centering
    \includegraphics[scale=0.35]{function_spec.png}
	\caption{Example of a function spec.}
	\label{fig:function_spec}
\end{figure}

Figure \ref{fig:process_assignment} depicts an executor submitting a function specification that is later assigned to another executor. When registering, executors have to specify to the Colonies server which functions they are capable of executing. The Colonies server then makes sure that the conditions (i.e requirmenets) part of the function specification matches the capability of an executor. 

\begin{equation}
    \label{eq:pt}
    priority_{time}=submission_{time} - priority \cdot 10^9 \cdot 60 \cdot 60 \cdot 24
\end{equation}

Table \ref{proctable} shows an example of a process table. The process table also contains a reference to a function specification depicted in Table \ref{functable}. To assign a process to an executor, the Colonies server searches in the process table to find a process matching a waiting executor. By using the \emph{priority time} column to sort the processes, the process table serves as a queue. This can be done in SQL by utilizing the \emph{order by} to sort processes according their submission time. To make it possible to handle priorites, the submission time is adjusted so that higher priority processes are processed before lower priority processes. When a process is submitted, a \emph{priority time} value is calculated and stored in the process table. Equation \ref{eq:pt} shows how the priority time is calculated for a nanosecond timestamp. 

\subsection{A stateless failsafe mechanism}
The primary objective of all Colonies API requests is to alter some state stored in the database or retrieve information from the database. The Colonies framework is designed to be stateless, meaning that the Colonies server does not keep any information between requests. In other words, each request is handled independently, without relying on any information from previous requests. 

Figure \ref{fig:function_spec} shows an example of a function specification. The \emph{maxexectime} attribute specifies the maximum time an executor may run a process (in this case, 100 seconds). Before a process is assigned to an executor, the Colonies server updates the process entry in the process table database and calculates a deadline when the process must be finished. The server then regularly checks for any running processes that have exceeded their deadlines. If such process is detected, it is reset, allowing it to be re-assigned to other executors. 

Making it possible to specify maximum execution time is a simple but powerful mechanism. To scale up a system, more executors can simply be deployed. Scaling down, however, can be more challenging. One solution is to select a set of executors to be removed and then starv them out by denying them new process assignments. Another, simpler solution, is to immediately destroy the executors and use the \emph{maxexectime} failsafe mechanism move back processes from defunct executors to the queue. The \emph{maxexectime} failsafe mechanism ensures that processes will eventually be executed even in the case of failures. This mechanism also relieves the burden of user to check if a process has been executed or not, as they can simply look up the process in the database to get its current status. 

Utilizing the \emph{maxexectime} failsafe mechanism not only enhances system reliability, but also provides an opportunity to apply Chaos engineering \cite{chaos_engineering}. For example, a Chaos monkey can be used to deliberately terminate executors. If executors are deployed on Kubernetes, Kubernetes will then automatically redeploy terminated executors. The constant flux of executor replacements ensures that the system is capable of gracefully tolerating failures.

\begin{figure}[h]
	\centering
    \includegraphics[scale=0.5]{raft.png}
	\caption{High-availability deployment of Colonies servcer.}
	\label{fig:ha_deployment}
\end{figure}

\subsubsection{Data consistency and distributed consensus}
Synchronization is esstential to prevent data inconsistency and race conditions when accessing shared resources concurrently with multiple threads. However, synchronization can also slows down execution as only one thread can access critical sections at a time. By carefully designing multithreaded applications and employing the right synchronization techniques, it possible to minimize the performance impact while still ensuring data consistency and correctness.  

The assign API request binds a process from the database to an executor. Given the multi-threaded nature of the Colonies server, it is essential that the assign request is synchronized to ensure that only one thread at a time can modify the database and update the process table, thus preventing multiple executors from being assigned to the same process. To ensure that only one executor can be assigned to a process, the assign request must be synchronization. It is worth noting that synchronization is not necessary for other requests. For example, as the submit request only add new entries to the process table, there is no race conditions. The close request set the output of the function innovation and updates the process state to either successful or failed in the process table. Since there can only be one executor assigned to a process there is no race conditions and consequently no need for synchronization.

To minimize downtime, Colonies supports high-availability deployments. If one Coloniser server crashes, an executor simply need to re-send the failed request, which will then be served by another Colonies server replica. However, by introducing multiple Colonies servers, there is again a risk of race conditions when assigning processes to executors. This means that the Colonies server replicas must coordintate which replica server incomming assign requests so that precisely one executor is assigned to a process.

Raft \cite{raft} is a consensus algorithm specifically designed to manage a replicated log within a distributed system. It functions within a cluster of servers, where a single server takes on the role of leader while the remaining servers act as followers. The leader is responsible for managing the replicated log, processing client requests, and replicating entries to the followers. Followers passively replicate the leader's log and participate in leader elections. The leadership can change over time due to elections triggered by timeouts or other factors.

Incorporting Raft with the Colonies framework allows incoming assign requests to be directed towards the leading Colonies server, thereby ensuring that only one Colonies server replica handles such requests. Figure \ref{fig:ha_deployment} shows an overview of a high-availability Colonies deployment. A new leader is elected in the event that a Colonies server replica fails. The Raft protocol also enables seamless updates to the Colonies server software by making it possible to upgrade each replica individually. Consequently, Colonies is well-suited for Kubernetes deployments, ensuring high levels of availability and fault tolerance.

\subsubsection{Workflows}
A workflow is a series of processes that need to be completed in a specific order. In Colonies, workflows are represented as directed acyclic graphs (DAGs) where nodes represent processes and edges represent dependencies and data flow between processes. Similar to processes, workflows are managed completely statelessly. When a workflow is submitted, all processes part of the workflow are submitted and added to the process table similar to how ordinary processes are handled. To control the order processes can be assigned, Colonies sets a flag, \emph{wait for parents} to prevent processes to be assigned to an executor before their dependencies have been satisfied. Note that processes may run in parallel if sufficient number of executors are available.

\begin{table}[h]
	\caption{Dependency Table}
	\centering
	\begin{tabular}{lll}
		\toprule
		\cmidrule(r){1-2}
        Process Id & Name       & Dependencies           \\
		\midrule
        $P_{1}$    & $Task_{1}$ & -                      \\
        $P_{2}$    & $Task_{2}$ & $Task_{1}$             \\
        $P_{3}$    & $Task_{3}$ & $Task_{1}$             \\
        $P_{4}$    & $Task_{4}$ & $Task_{2}$, $Task_{3}$ \\
		\bottomrule
	\end{tabular}
	\label{deptable}
\end{table}

\begin{figure}[h]
	\centering
    \includegraphics[scale=0.30]{workflow.png}
	\caption{Workflow execution timeline.}
	\label{fig:workflowexec}
\end{figure}

When a process is closed, the Colonies server checks its child processes to see if all their parent processes have also completed. If all parents of a child process have finished, the server updates the process table in the database by setting the flag \emph{wait for parents} to false, which allows child processes to be assigned to executors. This operation is also done statelessly when processing the close request. 

Figure \ref{fig:workflowexec} portrays an execution timeline for a workflow. Upon submission, process \(P_{1}\) is assigned to Executor \(E_{1}\). After being closed, processes \(P_{2}\) and \(P_{3}\) are simultaneously assigned to \(E_{1}\) and \(E_{2}\), allowing for concurrent execution. Lastly, process \(P_{4}\) is assigned to \(E_{2}\).  

To be able to generate a DAG, an additional database table is required. Table \ref{deptable} shows an example of a dependency table stroing relationship and dependencies between processes. The table is updated when a workflow is submitted to a Colonies server. As mentioned before, when an executor is assigned a process it obtains exclusive rights to that process within the process table. This exclusivity also means that is possible for an exector to dynamically add new children processes the workflow DAG on the fly. As a result, it becomes possible to implement MapReduce \cite{mapreduce} data processing patterns similar to Hadoop \cite{hadoop}.  

\begin{table}[h]
	\caption{Input/Output}
	\centering
	\begin{tabular}{lll}
		\toprule
		\cmidrule(r){1-2}
        Process Id & Input & Output \\
		\midrule
        $P_{1}$    & & [2,3] \\
        $P_{2}$    & 2 & 4 \\
        $P_{3}$    & 3 & 9 \\
        $P_{4}$    & [4,9] & 13 \\
		\bottomrule
	\end{tabular}
	\label{inouttable}
\end{table}

To manage data flow between processes that are part of a workflow, it is necessary to store the input and output values resulting from function invocations in a database. An example of such a database is presented in Table \ref{inouttable}. An executor can subsequently query the output from a parent process and use that as arguments while invoking a function.

\subsubsection{Cron}
TODO
\begin{figure}[h]
	\centering
    \includegraphics[scale=0.4]{cron.png}
	\caption{Sample figure caption.}
	\label{fig:fig1}
\end{figure}

\subsubsection{Generators}
TODO

\subsubsection{Zero-trust security}
\label{zerotrustsecurity}
TODO

\section{Use cases}
\subsection{Serverless computing}
\label{faas}

\section{Evaluation}
\subsection{Implementation}

\bibliographystyle{unsrt}
\bibliography{references} 

\end{document}
