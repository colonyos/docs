\documentclass{article}

\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{listings}

\title{Colonies - Compute Continuums across Platforms}

\author{{\hspace{1mm}Johan Kristiansson} \\
	Department of Computer Science \\
	RISE Research Institutes of Sweden \\
	Luleå, Sweden \\
	\texttt{johan.kristiansson@ri.se} \\
	\And
	{\hspace{1mm}Thomas Ohlson Timoudas} \\
	Department of Computer Science \\
	RISE Research Institutes of Sweden \\
	Luleå, Sweden \\
	\texttt{thomas.ohlson.timoudas@ri.se} \\
	\And
	{\hspace{1mm}Henrik Forsgren} \\
	Department of Computer Science \\
	RISE Research Institutes of Sweden \\
	Luleå, Sweden \\
	\texttt{thomas.ohlson.timoudas@ri.se} \\
}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
Running AI/ML models in production is becoming widespread. At the same time, developing and maintaining AI workloads are becoming more difficult. In particular, most workloads are not portable and cannot easily be moved from one provider to another. Creating and operating fully automated end-to-end workflows across devices, edge, cloud adds even more complexity.

This paper presents a novel framework for running computational workload across heterogeneous platforms. Colonies is based on a loosely coupled microservice architecture where complex workflows are broken down in composable functions that are executed by independently deployable executors. Using a HTTP protocol, functions can be composed into declarative workflows in any computer language. The workflows are then executed across platforms by independently deployed executors running in the cloud, edge, devices, or even in web browser, creating compute continuums across platforms. Colonies support both real-time processing and batch jobs while at the same time offer full traceability and zero-trust security.

In additional to a technical description, the paper also describes how Colonies can be used to build a scalable remote sensing platform on Kubernetes, how it can be used as a building block for edge computing, and how it can be integrated with HPC platforms. Finally, the paper presents a performance investigation as well as a scalability and robustness evaluation. 
\end{abstract}

% keywords can be removed
\keywords{Serverless computing \and Parallel computing \and Workflow orchestration}

\section{Introduction}
Building robust and scalable AI systems is challenging and requires a deep understanding of various fields. Firstly, an AI model must be trained, which requires technical skills in advanced statistics or machine learning, as well as access to training and validation data. Usually, the data needs to be pre-processed in several steps before it can be used, or a simulator needs to be developed to generate synthetic data or play back historical data. While it may be reasonable for small-scale projects to run the whole environment on local development computers, training larger AI models usually requires access to powerful compute clusters or even HPC systems. Manually utilizing such infrastructure is cumbersome and time-consuming. Automating the training processes makes it possible to iterate more quickly and find useful models.

Going forward and taking an AI model into production requires significant software engineering skills. In contrast to traditional IT workloads, both the data and the model itself need to be managed. As most models need to be re-trained or re-calibrated regularly, it must be possible to seamlessly update the deployed model and the software without losing information or introducing delays. In many cases, there is a constant flow of data that is ingested into the system that needs to be managed while parts of the system are not working correctly. This becomes even more challenging when nodes or parts of the underlying infrastructure crash or become unavailable due to maintenance, such as software updates or misconfiguration errors.

Sometimes there is also need to scale the system to increase capacity or scale down to save resources. This is particularly important when using expensive cloud resources. Scaling the system means that underlying infastructure may change anytime causing instability problems for running applications. It must therefor be possible to detected failed computations and re-process failed tasks part of a larger workflow. Running workflows must therefor be designed to handle an ever changing infrastructure, and if a failed computation cannot gracefully be restored, there must be a away for engineers perform root cause analysis and manually recover failures.

In reality, several systems must be integrated to build fully working AI applications. For example, data might must be captured from an IoT system or pulled from third-party services running on different domains than the compute cluster. With the introduction of edge computing, parts of the data pipeline might also execute on edge servers, or even in on devices to bring computation closer where the data is produced. Configuring and setting up such pipelines adds further complexity. 

In addition, many compute clusters run on-prem and somtimes there is a need to temporary increase the capacity by mixing on-prem and cloud resources, for example to handle peek load or re-process historical data. This also requires integration of various security protocols to provision and . Creating hybrid workflows where some jobs run on HPC systems requires even more software development efforts and is out of the scope for many users, preventing them from utilizing powerful hardware. Clearly, there is a need for a framework that can consolidate various platforms to simplify development and allowing computations to seamlessly run across platforms.

This paper presents a loosly coupled microservice inspired architecuture that seperates workflow definitions from implementation and deployment. The ultimate goal is to create an architecuture where monolitic workflows are broken down into independent and seperated compute units, which can be dynamically added or removed to running workflows. These compute units could then be implemented in any computer language and be deployed anywhere on the Internet. The paper describes a framework called Colonies implementing such an architecutre. The paper describes several use cases   

Taking an AI model intro production, 

Development
 - Data science
 - Production system
 - Integration, data ingestion, pre-processing
 Workflows, real-time processing, back processing.
Operation
 - Computer faults
 - Updates, DevOps
Scability
HPC
 Requires a whole different set of toolchains. 

 


https://modelserving.com/blog/why-do-people-say-its-so-hard-to-deploy-a-ml-model-to-production
\section{The Colonies framework}
\label{sec:headings}

\subsection{Architecture}
TODO
\begin{figure}[h]
	\centering
    \includegraphics[scale=0.4]{arch.png}
	\caption{cron management}
	\label{fig:fig1}
\end{figure}

\begin{figure}[h]
	\centering
    \includegraphics[scale=0.4]{raft.png}
	\caption{cron management}
	\label{fig:fig1}
\end{figure}

\subsubsection{Workflows}
TODO
\begin{figure}[h]
	\centering
    \includegraphics[scale=0.32]{workflow.png}
	\caption{cron management}
	\label{fig:fig1}
\end{figure}

\begin{table}[h]
	\caption{Function Specifications}
	\centering
	\begin{tabular}{llllll}
		\toprule
		\cmidrule(r){1-2}
        Function Spec & Function        & Executor Type & Priority & Max Exec Time & Max Retries \\
		\midrule
        $F_{1}$       & gen\_nums()     & Edge          & 1        & 200 s         & 5 \\
        $F_{2}$       & square()        & Cloud         & 1        & 200 s         & 5 \\
        $F_{3}$       & square()        & Cloud         & 1        & 200 s         & 5 \\
        $F_{4}$       & sum()           & Browser       & 1        & 200 s         & 5 \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}

\begin{table}[h]
	\caption{Snapshot of Process Table as in Step 2}
	\centering
	\begin{tabular}{llllll}
		\toprule
		\cmidrule(r){1-2}
        Process Id & Function Spec & Wait for Parents & Assigned Executor Id & State      & Priority Time \\
		\midrule
        $P_{1}$    & $F_{1}$       & $False$          & $E_{1}$              & Successful & 1679906715352024000 \\
        $P_{2}$    & $F_{2}$       & $False$          & $E_{1}$              & Running    & 1679906715353453000 \\
        $P_{3}$    & $F_{3}$       & $False$          & $E_{2}$              & Running    & 1679906715354286000 \\
        $P_{4}$    & $F_{4}$       & $True$           & -                    & Waiting    & 1679906715355188000 \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}

dt = -1000000000 * 60 * 60 * 24
process.PriorityTime = int64(process.FunctionSpec.Priority)*dt + submissionTime.UnixNano()


\begin{table}[h]
	\caption{Dependency Table}
	\centering
	\begin{tabular}{lll}
		\toprule
		\cmidrule(r){1-2}
        Process Id & Name       & Dependencies           \\
		\midrule
        $P_{1}$    & $Task_{1}$ & -                      \\
        $P_{2}$    & $Task_{2}$ & $Task_{1}$             \\
        $P_{3}$    & $Task_{3}$ & $Task_{1}$             \\
        $P_{4}$    & $Task_{4}$ & $Task_{2}$, $Task_{3}$ \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}
	
\begin{table}[h]
	\caption{Input/Output Table}
	\centering
	\begin{tabular}{lll}
		\toprule
		\cmidrule(r){1-2}
        Process Id & Input & Output \\
		\midrule
        $P_{1}$    & & [2,3] \\
        $P_{2}$    & 2 & 4 \\
        $P_{3}$    & 3 & 9 \\
        $P_{4}$    & [4,9] & 13 \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}

\subsubsection{Cron}
TODO
\begin{figure}[h]
	\centering
    \includegraphics[scale=0.4]{cron.png}
	\caption{Sample figure caption.}
	\label{fig:fig1}
\end{figure}

\subsubsection{Generators}
TODO

\subsubsection{Zero-trust security}
TODO

\section{Evaluation}
\subsection{Implementation}
\begin{lstlisting}[language=c]
gen_nums = Function(gen_data, colonyid, executortype="edge")
square1 = Function(square, colonyid, executortype="cloud")
square2 = Function(square, colonyid, executortype="cloud")
sum = Function(square, colonyid, executortype="browser")

wf = ColoniesWorkflow("localhost", 50080, colonyid, executor_prvkey)
wf >> gennums
gennums >> square1
gennums >> square2
[square1, square2] >> sum
res = wf.execute()
\end{lstlisting}

\subsection{References}
TODO
\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}

\end{document}
